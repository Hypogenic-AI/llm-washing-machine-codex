\section{Abstract}
We ask whether the compound concept ``washing machine'' is represented in a transformer residual stream by a distinct feature or by composition of its constituents. We analyze \gpttwo using a pretrained \sae at layer 6 (\texttt{resid\_post\_mlp}) and contexts mined from \wikitext. Our study combines three signals: (1) top-$k$ overlap of \sae features activated by compound and constituent contexts, (2) causal patching of residual activations to test whether compound activations lift the ``machine'' logit, and (3) a compositionality probe that predicts compound embeddings from constituent embeddings. We find low feature overlap between compound and constituents (Jaccard 0.11--0.14; bootstrap means 0.09--0.11), no reliable causal lift from patching (mean $\Delta$logit $-0.019 \pm 0.109$, $p=0.75$), and extremely strong compositional predictability (ridge probe mean cosine 0.996; MSE 5.19 vs 12.49 for a \wbaseline baseline). These results favor distributed composition over a single dominant compound-specific direction at the tested layer. The findings inform interpretability and editing methods that assume atomic concept directions, suggesting that compound concepts may require multi-feature interventions.
