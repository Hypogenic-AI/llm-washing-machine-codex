\begin{thebibliography}{7}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anthropic(2023)]{anthropic2023monosemanticity}
Anthropic.
\newblock Towards monosemanticity: Decomposing language models with dictionary
  learning.
\newblock \emph{Transformer Circuits Report}, 2023.

\bibitem[Cunningham et~al.(2023)Cunningham, Ewart, Riggs, Huben, and
  Sharkey]{cunningham2023sparse}
Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey.
\newblock Sparse autoencoders find highly interpretable features in language
  models.
\newblock \emph{arXiv preprint arXiv:2309.08600}, 2023.

\bibitem[Dai et~al.(2021)]{dai2021knowledgeneurons}
Damai Dai et~al.
\newblock Knowledge neurons in pretrained transformers.
\newblock \emph{arXiv preprint arXiv:2104.08696}, 2021.

\bibitem[Elhage et~al.(2022)Elhage, Hume, Olsson, Schaeffer, Henighan, and
  Olah]{elhage2022superposition}
Nelson Elhage, Tristan Hume, Catherine Olsson, Brandon Schaeffer, Tom Henighan,
  and Chris Olah.
\newblock Toy models of superposition.
\newblock \emph{arXiv preprint arXiv:2209.10652}, 2022.

\bibitem[Meng et~al.(2022)]{meng2022rome}
Kevin Meng et~al.
\newblock Locating and editing factual associations in {GPT}.
\newblock \emph{arXiv preprint arXiv:2202.05262}, 2022.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and
  Socher]{merity2017wikitext}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Wang et~al.(2022)]{wang2022ioi}
Kevin Wang et~al.
\newblock Interpretability in the wild: a circuit for indirect object
  identification in {GPT-2} small.
\newblock \emph{arXiv preprint arXiv:2211.00593}, 2022.

\end{thebibliography}
