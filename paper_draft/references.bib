@article{elhage2022superposition,
  title={Toy Models of Superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schaeffer, Brandon and Henighan, Tom and Olah, Chris},
  year={2022},
  journal={arXiv preprint arXiv:2209.10652}
}

@article{liu2022groundup,
  title={Are Representations Built from the Ground Up? An Empirical Examination of Local Composition in Language Models},
  author={Liu, Emmy and Neubig, Graham},
  year={2022},
  journal={arXiv preprint arXiv:2210.03575}
}

@article{zhang2023patching,
  title={Towards Best Practices of Activation Patching in Language Models: A Systematic Survey},
  author={Zhang, Fred and Nanda, Neel},
  year={2023},
  journal={arXiv preprint arXiv:2309.16042}
}

@article{park2024linear,
  title={The Linear Representation Hypothesis and the Geometry of Large Language Models},
  author={Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  year={2024},
  journal={arXiv preprint arXiv:2311.03658}
}

@article{gao2024sae,
  title={Scaling and Evaluating Sparse Autoencoders},
  author={Gao, Leo and others},
  year={2024},
  journal={arXiv preprint arXiv:2406.04093}
}

@article{leask2025sae,
  title={Sparse Autoencoders Do Not Find Canonical Units of Analysis},
  author={Leask, Patrick and others},
  year={2025},
  journal={arXiv preprint arXiv:2502.04878}
}

@article{heap2025metrics,
  title={Automated Interpretability Metrics Do Not Distinguish Trained and Random Transformers},
  author={Heap, Thomas and others},
  year={2025},
  journal={arXiv preprint arXiv:2501.17727}
}

@article{lawson2025mlsae,
  title={Residual Stream Analysis with Multi-Layer Sparse Autoencoders},
  author={Lawson, Tim and others},
  year={2025},
  journal={arXiv preprint arXiv:2409.04185}
}
