# Downloaded Papers

1. Toy Models of Superposition (2022)
   - Authors: Nelson Elhage et al.
   - arXiv: 2209.10652
   - File: 2209.10652_toy_models_of_superposition.pdf
   - Why relevant: Formalizes superposition/polysemanticity, central to how concepts might be encoded in residual streams.

2. Sparse Autoencoders Find Highly Interpretable Features in Language Models (2023)
   - Authors: Hoagy Cunningham et al.
   - arXiv: 2309.08600
   - File: 2309.08600_sparse_autoencoders_interpretable_features.pdf
   - Why relevant: Uses SAEs to find sparse directions in activations, directly addressing concept-level features.

3. Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small (2022)
   - Authors: Kevin Wang et al.
   - arXiv: 2211.00593
   - File: 2211.00593_interpretability_in_the_wild.pdf
   - Why relevant: Demonstrates causal tracing of a specific behavior in GPT-2 small; methodology useful for locating concept circuits.

4. Locating and Editing Factual Associations in GPT (2022)
   - Authors: Kevin Meng et al.
   - arXiv: 2202.05262
   - File: 2202.05262_locating_and_editing_factual_associations.pdf
   - Why relevant: Shows factual associations localized in mid-layer MLPs; relevant to locating concept storage.

5. Knowledge Neurons in Pretrained Transformers (2021)
   - Authors: Damai Dai et al.
   - arXiv: 2104.08696
   - File: 2104.08696_knowledge_neurons.pdf
   - Why relevant: Proposes neuron-level attribution for factual knowledge; related to single-concept localization.
