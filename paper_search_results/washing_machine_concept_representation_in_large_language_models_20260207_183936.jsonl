{"title": "CLMASP: Coupling Large Language Models with Answer Set Programming for Robotic Task Planning", "year": 2024, "authors": "Xinrui Lin, Yang-Chang Wu, Huanyu Yang, Yu Zhang, Yanyong Zhang, Jianmin Ji", "url": "https://api.semanticscholar.org/CorpusId:270258182", "relevance": 2, "abstract": "Large Language Models (LLMs) possess extensive foundational knowledge and moderate reasoning abilities, making them suitable for general task planning in open-world scenarios. However, it is challenging to ground a LLM-generated plan to be executable for the specified robot with certain restrictions. This paper introduces CLMASP, an approach that couples LLMs with Answer Set Programming (ASP) to overcome the limitations, where ASP is a non-monotonic logic programming formalism renowned for its capacity to represent and reason about a robot's action knowledge. CLMASP initiates with a LLM generating a basic skeleton plan, which is subsequently tailored to the specific scenario using a vector database. This plan is then refined by an ASP program with a robot's action knowledge, which integrates implementation details into the skeleton, grounding the LLM's abstract outputs in practical robot contexts. Our experiments conducted on the VirtualHome platform demonstrate CLMASP's efficacy. Compared to the baseline executable rate of under 2% with LLM approaches, CLMASP significantly improves this to over 90%.", "citations": 11}
{"title": "Human-like object concept representations emerge naturally in multimodal large language models", "year": 2024, "authors": "Changde Du, Kaicheng Fu, Bincheng Wen, Yi Sun, Jie Peng, Wei Wei, Ying Gao, Shengpei Wang, Chuncheng Zhang, Jinpeng Li, Shuang Qiu, Le Chang, Huiguang He", "url": "https://api.semanticscholar.org/CorpusId:270870140", "relevance": 1, "abstract": "Understanding how humans conceptualize and categorize natural objects offers critical insights into perception and cognition. With the advent of large language models (LLMs), a key question arises: can these models develop human-like object representations from linguistic and multimodal data? Here we combined behavioural and neuroimaging analyses to explore the relationship between object concept representations in LLMs and human cognition. We collected 4.7 million triplet judgements from LLMs and multimodal LLMs to derive low-dimensional embeddings that capture the similarity structure of 1,854 natural objects. The resulting 66-dimensional embeddings were stable, predictive and exhibited semantic clustering similar to human mental representations. Remarkably, the dimensions underlying these embeddings were interpretable, suggesting that LLMs and multimodal LLMs develop human-like conceptual representations of objects. Further analysis showed strong alignment between model embeddings and neural activity patterns in brain regions such as the extrastriate body area, parahippocampal place area, retrosplenial cortex and fusiform face area. This provides compelling evidence that the object representations in LLMs, although not identical to human ones, share fundamental similarities that reflect key aspects of human conceptual knowledge. Our findings advance the understanding of machine intelligence and inform the development of more human-like artificial cognitive systems. Multimodal large language models are shown to develop object concept representations similar to those of humans. These representations closely align with neural activity in brain regions involved in object recognition, revealing similarities between artificial intelligence and human cognition.", "citations": 23}
{"title": "PaCE: Parsimonious Concept Engineering for Large Language Models", "year": 2024, "authors": "Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, D. Thaker, Aditya Chattopadhyay, Chris Callison-Burch, Ren'e Vidal", "url": "https://api.semanticscholar.org/CorpusId:270285564", "relevance": 1, "abstract": "Large Language Models (LLMs) are being used for a wide variety of tasks. While they are capable of generating human-like responses, they can also produce undesirable output including potentially harmful information, racist or sexist language, and hallucinations. Alignment methods are designed to reduce such undesirable outputs via techniques such as fine-tuning, prompt engineering, and representation engineering. However, existing methods face several challenges: some require costly fine-tuning for every alignment task; some do not adequately remove undesirable concepts, failing alignment; some remove benign concepts, lowering the linguistic capabilities of LLMs. To address these issues, we propose Parsimonious Concept Engineering (PaCE), a novel activation engineering framework for alignment. First, to sufficiently model the concepts, we construct a large-scale concept dictionary in the activation space, in which each atom corresponds to a semantic concept. Given any alignment task, we instruct a concept partitioner to efficiently annotate the concepts as benign or undesirable. Then, at inference time, we decompose the LLM activations along the concept dictionary via sparse coding, to accurately represent the activations as linear combinations of benign and undesirable components. By removing the latter ones from the activations, we reorient the behavior of the LLM towards the alignment goal. We conduct experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising, and show that PaCE achieves state-of-the-art alignment performance while maintaining linguistic capabilities.", "citations": 16}
{"title": "On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe", "year": 2024, "authors": "Ningyu Xu, Qi Zhang, Menghan Zhang, Peng Qian, Xuanjing Huang", "url": "https://api.semanticscholar.org/CorpusId:267782727", "relevance": 1, "abstract": "Probing and enhancing large language models' reasoning capacity remains a crucial open question. Here we re-purpose the reverse dictionary task as a case study to probe LLMs' capacity for conceptual inference. We use in-context learning to guide the models to generate the term for an object concept implied in a linguistic description. Models robustly achieve high accuracy in this task, and their representation space encodes information about object categories and fine-grained features. Further experiments suggest that the conceptual inference ability as probed by the reverse-dictionary task predicts model's general reasoning performance across multiple benchmarks, despite similar syntactic generalization behaviors across models. Explorative analyses suggest that prompting LLMs with description$\\Rightarrow$word examples may induce generalization beyond surface-level differences in task construals and facilitate models on broader commonsense reasoning problems.", "citations": 5}
{"title": "Conceptual structure coheres in human cognition but not in large language models", "year": 2023, "authors": "Siddharth Suresh, Kushin Mukherjee, Xizheng Yu, Wei-Chun Huang, Lisa Padua, T. Rogers", "url": "https://www.semanticscholar.org/paper/3a69769d2d0d259299373698ae73c940a255e932", "relevance": 1, "abstract": "Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior in various semantic tasks. Contemporary large language models (LLMs), however, make it possible to interrogate the latent structure of conceptual representations using experimental methods nearly identical to those commonly used with human participants. The current work utilizes three common techniques borrowed from cognitive psychology to estimate and compare the structure of concepts in humans and a suite of LLMs. In humans, we show that conceptual structure is robust to differences in culture, language, and method of estimation. Structures estimated from LLM behavior, while individually fairly consistent with those estimated from human behavior, vary much more depending upon the particular task used to generate responses--across tasks, estimates of conceptual structure from the very same model cohere less with one another than do human structure estimates. These results highlight an important difference between contemporary LLMs and human cognition, with implications for understanding some fundamental limitations of contemporary machine language.", "citations": 14}
{"title": "Revealing emergent human-like conceptual representations from language prediction.", "year": 2025, "authors": "Ningyu Xu, Qi Zhang, Chao Du, Qiang Luo, Xipeng Qiu, Xuanjing Huang, Menghan Zhang", "url": "https://api.semanticscholar.org/CorpusId:275788182", "relevance": 1, "abstract": "People acquire concepts through rich physical and social experiences and use them to understand and navigate the world. In contrast, large language models (LLMs), trained solely through next-token prediction on text, exhibit strikingly human-like behaviors. Are these models developing concepts akin to those of humans? If so, how are such concepts represented, organized, and related to behavior? Here, we address these questions by investigating the representations formed by LLMs during an in-context concept inference task. We found that LLMs can flexibly derive concepts from linguistic descriptions in relation to contextual cues about other concepts. The derived representations converge toward a shared, context-independent structure, and alignment with this structure reliably predicts model performance across various understanding and reasoning tasks. Moreover, the convergent representations effectively capture human behavioral judgments and closely align with neural activity patterns in the human brain, providing evidence for biological plausibility. Together, these findings establish that structured, human-like conceptual representations can emerge purely from language prediction without real-world grounding, highlighting the role of conceptual structure in understanding intelligent behavior. More broadly, our work suggests that LLMs offer a tangible window into the nature of human concepts and lays the groundwork for advancing alignment between artificial and human intelligence.", "citations": 2}
{"title": "Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era", "year": 2025, "authors": "Dan Onea\u0163\u0103, Desmond Elliott, Stella Frank", "url": "https://api.semanticscholar.org/CorpusId:279154538", "relevance": 1, "abstract": "Human learning and conceptual representation is grounded in sensorimotor experience, in contrast to state-of-the-art foundation models. In this paper, we investigate how well such large-scale models, trained on vast quantities of data, represent the semantic feature norms of concrete object concepts, e.g. a ROSE is red, smells sweet, and is a flower. More specifically, we use probing tasks to test which properties of objects these models are aware of. We evaluate image encoders trained on image data alone, as well as multimodally-trained image encoders and language-only models, on predicting an extended denser version of the classic McRae norms and the newer Binder dataset of attribute ratings. We find that multimodal image encoders slightly outperform language-only approaches, and that image-only encoders perform comparably to the language models, even on non-visual attributes that are classified as\"encyclopedic\"or\"function\". These results offer new insights into what can be learned from pure unimodal learning, and the complementarity of the modalities.", "citations": 3}
{"title": "Testing Causal Models of Word Meaning in GPT-3 and -4", "year": 2023, "authors": "Sam Musker, Ellie Pavlick", "url": "https://api.semanticscholar.org/CorpusId:258865763", "relevance": 1, "abstract": "Large Language Models (LLMs) have driven extraordinary improvements in NLP. However, it is unclear how such models represent lexical concepts-i.e., the meanings of the words they use. This paper evaluates the lexical representations of GPT-3 and GPT-4 through the lens of HIPE theory, a theory of concept representations which focuses on representations of words describing artifacts (such as\"mop\",\"pencil\", and\"whistle\"). The theory posits a causal graph that relates the meanings of such words to the form, use, and history of the objects to which they refer. We test LLMs using the same stimuli originally used by Chaigneau et al. (2004) to evaluate the theory in humans, and consider a variety of prompt designs. Our experiments concern judgements about causal outcomes, object function, and object naming. We find no evidence that GPT-3 encodes the causal structure hypothesized by HIPE, but do find evidence that GPT-4 encodes such structure. The results contribute to a growing body of research characterizing the representational capacity of large language models.", "citations": 1}
{"title": "CLIP-QDA: An Explainable Concept Bottleneck Model", "year": 2023, "authors": "R\u00e9mi Kazmierczak, Eloise Berthier, Goran Frehse, Gianni Franchi", "url": "https://api.semanticscholar.org/CorpusId:265552032", "relevance": 1, "abstract": "In this paper, we introduce an explainable algorithm designed from a multi-modal foundation model, that performs fast and explainable image classification. Drawing inspiration from CLIP-based Concept Bottleneck Models (CBMs), our method creates a latent space where each neuron is linked to a specific word. Observing that this latent space can be modeled with simple distributions, we use a Mixture of Gaussians (MoG) formalism to enhance the interpretability of this latent space. Then, we introduce CLIP-QDA, a classifier that only uses statistical values to infer labels from the concepts. In addition, this formalism allows for both local and global explanations. These explanations come from the inner design of our architecture, our work is part of a new family of greybox models, combining performances of opaque foundation models and the interpretability of transparent models. Our empirical findings show that in instances where the MoG assumption holds, CLIP-QDA achieves similar accuracy with state-of-the-art methods CBMs. Our explanations compete with existing XAI methods while being faster to compute.", "citations": 10}
{"title": "From Representation to Response: Assessing the Alignment of Large Language Models with Human Judgment Patterns", "year": 2024, "authors": "Anastasiia Hrytsyna, R. Alves", "url": "https://www.semanticscholar.org/paper/8ac2295a9e3720cb2da9f49df531359cd442d402", "relevance": 1, "abstract": "Large language models (LLMs) are sophisticated artificial intelligence systems designed to process and understand natural language at a complex level. The recent progress of these models, culminating in chat-based LLMs, has democratized the accessibility of these sophisticated intelligent systems, showcasing how machine learning methods can help humans in daily tasks. This research addresses the growing interest in understanding the mechanisms of LLMs and in evaluating their alignment with human cognition. We introduce an innovative alignment assessment strategy in the realm of LLMs that diverges from traditional approaches, utilizing the odd-one-out triplet-based task to investigate the alignment of LLMs\u2019 representations with human object concept mental organization. Our methodology, which incorporates image captioning and zero/few-shot learning accuracy scoring, is designed to evaluate language models\u2019 ability to predict similarities and differences in object concepts. A comprehensive experimental evaluation was conducted, involving four captioning strategies, twenty-four LLMs across eight model families, and three scoring procedures, utilizing a significantly large dataset for enhanced understanding of LLM comprehensibility. Finally, our study explores the impact of object description comprehensiveness on model-human representation alignment and analyzes a subset of randomly selected triplets to assess how LLMs are able to represent different levels of human judgment patterns.", "citations": 2}
{"title": "Continually Learn to Map Visual Concepts to Large Language Models in Resource-constrained Environments", "year": 2024, "authors": "Clea Rebillard, J. Hurtado, Andrii Krutsylo, Lucia C. Passaro, Vincenzo Lomonaco", "url": "https://api.semanticscholar.org/CorpusId:271098069", "relevance": 1, "abstract": "", "citations": 0}
{"title": "Precise In-Parameter Concept Erasure in Large Language Models", "year": 2025, "authors": "Yoav Gur-Arieh, Clara Suslik, Yihuai Hong, Fazl Barez, Mor Geva", "url": "https://api.semanticscholar.org/CorpusId:278959968", "relevance": 1, "abstract": "Large language models (LLMs) often acquire knowledge during pretraining that is undesirable in downstream deployments, e.g., sensitive information or copyrighted content. Existing approaches for removing such knowledge rely on fine-tuning, training low-rank adapters or fact-level editing, but these are either too coarse, too shallow, or ineffective. In this work, we propose PISCES (Precise In-parameter Suppression for Concept EraSure), a novel framework for precisely erasing entire concepts from model parameters by directly editing directions that encode them in parameter space. PISCES uses a disentangler model to decompose MLP vectors into interpretable features, identifies those associated with a target concept using automated interpretability techniques, and removes them from model parameters. Experiments on Gemma 2 and Llama 3.1 over various concepts show that PISCES achieves modest gains in efficacy over leading erasure methods, reducing accuracy on the target concept to as low as 7.7%, while dramatically improving erasure specificity (by up to 31%) and robustness (by up to 38%). Overall, these results demonstrate that feature-based in-parameter editing enables a more precise and reliable approach for removing conceptual knowledge in language models.", "citations": 6}
