You are an academic paper writer. Generate a complete NEURIPS style paper
based on the experiment results provided.

════════════════════════════════════════════════════════════════════════════════
                         IMPORTANT: BEFORE YOU START
════════════════════════════════════════════════════════════════════════════════

Before writing any content, you MUST complete these steps:

1. READ THE SKILL: Review the paper-writer skill at templates/skills/paper-writer/SKILL.md
2. READ THE STYLE GUIDE: Study templates/paper_writing/lab_style_guide.md carefully
3. REVIEW EXAMPLES: Browse paper_examples/ for formatting and language patterns:
   - Look at sections/1.introduction.tex for language style
   - Look at tables/*.tex for table formatting
   - Look at commands/*.tex for macro usage
4. USE COMMAND TEMPLATES: Copy templates/paper_writing/commands/ to paper_draft/commands/

CRITICAL: Reference example papers for FORMATTING and LANGUAGE STYLE only.
Do NOT copy content, phrasing, or narrative structure from the example papers.
The examples are in a different research domain - focus only on presentation style.

════════════════════════════════════════════════════════════════════════════════
                            EXPERIMENT REPORT
════════════════════════════════════════════════════════════════════════════════

# REPORT: Where Is &#34;Washing Machine&#34; Stored in LLMs?

## 1. Executive Summary
**Research question**: Where is the compound concept &#34;washing machine&#34; represented in a transformer LLM residual stream—via a distinct feature or via composition of constituent concepts (&#34;washing&#34; + &#34;machine&#34;)?

**Key finding**: In GPT-2 small with SAE features at layer 6 (resid_post_mlp), top-k compound feature overlap with constituents is low (Jaccard 0.11–0.14; bootstrap means 0.09–0.11), but a compositionality probe predicts compound embeddings from constituents extremely well (mean cosine 0.996). Causal patching showed no reliable positive logit lift for &#34;machine&#34; (mean Δlogit -0.019, p=0.75).

**Implication**: The evidence favors compositional representation over a single strong compound-specific direction at the tested layer. Mechanistic interventions should expect compound concepts to be distributed across constituent features rather than localized to a single monosemantic direction.

## 2. Goal
**Hypothesis**: The compound concept &#34;washing machine&#34; is not stored as a unique orthogonal direction; it emerges from constituent features.

**Why it matters**: Interpretability and model editing methods often assume distinct concept directions. If compounds are compositional, then single-direction edits and interventions may be incomplete or misleading.

## 3. Data Construction

### Dataset Description
- **Source**: WikiText-2 raw (`datasets/wikitext_2_raw_v1`)
- **Size used**: 29,119 non-empty text lines after filtering
- **Task**: Context mining for compound and constituent mentions
- **Biases/limitations**: Sparse coverage of the specific compound; contexts may be domain-specific to Wikipedia

### Example Samples
**Compound contexts**:
- &#34;The washing machine was broken.&#34;
- &#34;The washing machine stopped mid-cycle.&#34;
- &#34;The washing machine made a loud noise.&#34;

**Washing-only contexts**:
- &#34;In the 1880s , the federal government began closing many small arsenals around the country in favor of smaller ones built near railroads for quick deployment .&#34;
- &#34;Atlanta was easily pulled free by the Union ships and she reached Port Royal under her own power .&#34;
- &#34;Michael Jeffrey Jordan ( born February 17 , 1963 ) , also known by his initials , MJ , is an American retired professional basketball player .&#34;

**Machine-only contexts**:
- &#34;Most of the equipment , arms , and machinery at the Little Rock Arsenal was removed to east of the Mississippi River by order of Maj. Gen. Earl Van Dorn in April and May 1862 ...&#34;
- &#34;Machinery was made for manufacturing percussion caps and small arms , and both were turned out in small quantity , but of excellent quality .&#34;
- &#34;The fourth stage involved remedying the problem of communicating between the two towers during the time of Pope Pius X.&#34;

### Data Quality
- **Total rows after filtering**: 29,119
- **Empty rows after filtering**: 0
- **Context counts**:
  - Compound: 105
  - Washing-only: 178
  - Machine-only: 200
- **Latent samples** (token positions found):
  - Compound: 105
  - Washing-only: 10
  - Machine-only: 164

### Preprocessing Steps
1. Load WikiText-2 raw from disk.
2. Strip whitespace and drop empty lines.
3. Extract three context sets:
   - Compound: lines containing &#34;washing machine&#34;
   - Washing-only: lines containing &#34;washing&#34; but not &#34;machine&#34;
   - Machine-only: lines containing &#34;machine&#34; but not &#34;washing&#34;
4. Use all splits as a pooled corpus for context search.

### Train/Val/Test Splits
No supervised training on WikiText-2. For the compositionality probe, an 80/20 random split of bigram samples was used.

## 4. Experiment Description

### Methodology
#### High-Level Approach
1. **SAE feature analysis**: Measure overlap between top-k SAE features activated by compound vs constituent contexts.
2. **Causal patching**: Patch residual activations from one prompt into another to test causal influence on predicting &#34;machine&#34;.
3. **Compositionality probe**: Train a ridge regression model to predict a compound embedding from constituent embeddings.

#### Why This Method?
- SAE features allow a sparse, interpretable basis for comparing concept activations.
- Causal patching tests whether a localized activation pattern causally induces compound-related logits.
- Probing quantifies how well compound representations are reconstructible from constituents.

### Implementation Details
#### Tools and Libraries
- PyTorch 2.10.0+cu128
- TransformerLens 2.15.4
- Transformers 4.57.6
- Datasets 4.5.0
- scikit-learn 1.8.0
- blobfile 3.2.0
- matplotlib 3.10.8

#### Algorithms/Models
- **Model**: GPT-2 small (`gpt2`) loaded via TransformerLens
- **SAE**: OpenAI `sparse_autoencoder` pretrained SAE (v5 32k) at layer 6, `resid_post_mlp`

#### Hyperparameters
| Parameter | Value | Selection Method |
|-----------|-------|------------------|
| SAE layer | 6 | Standard mid-layer choice |
| SAE location | resid_post_mlp | SAE default location |
| top_k | 50 | Common overlap eval setting |
| max_contexts | 200 | Runtime cap |
| ridge alpha | 1.0 | Baseline default |
| bootstrap samples | 200 | Quick CI estimate |

#### Training/Analysis Pipeline
1. Extract contexts from WikiText-2.
2. Run GPT-2 with cache to collect residual activations.
3. Encode activations with SAE to obtain latent features.
4. Compute top-k feature overlap and cosine similarities.
5. Run causal patching at layer 6 with template pairs.
6. Train ridge regression probe on bigram dataset to predict compound embeddings.

### Experimental Protocol
- **Random seed**: 42
- **Hardware**: NVIDIA GeForce RTX 3090 (24GB), CUDA 12.8
- **Runs**: 1 (deterministic, with bootstrap for overlap confidence intervals)

### Evaluation Metrics
- **Top-k Jaccard overlap**: feature overlap between conditions
- **Cosine similarity**: similarity of mean latent vectors
- **Causal patching logit delta**: impact on predicting &#34;machine&#34; after patching
- **Probe MSE / cosine**: quality of reconstructing compound embeddings

### Raw Results
#### Tables
| Metric | Value |
|--------|-------|
| Compound–washing Jaccard | 0.136 |
| Compound–machine Jaccard | 0.111 |
| Compound–union Jaccard | 0.129 |
| Compound unique fraction (top-50) | 0.68 |
| Bootstrap mean overlap (compound–washing) | 0.092 [0.064, 0.136] |
| Bootstrap mean overlap (compound–machine) | 0.105 [0.087, 0.124] |
| Bootstrap mean overlap (compound–union) | 0.114 [0.092, 0.137] |
| Cosine(compound, washing) | 0.578 |
| Cosine(compound, machine) | 0.041 |
| Causal patching Δlogit | -0.019 ± 0.109 (n=5, p=0.750) |
| Probe MSE (ridge) | 5.19 |
| Probe MSE (w2 baseline) | 12.49 |
| Probe mean cosine (ridge) | 0.996 |

#### Visualizations
- `results/plots/sae_overlap.png`
- `results/plots/causal_patching.png`

#### Output Locations
- Metrics JSON: `results/metrics.json`
- Examples JSON: `results/examples.json`
- Plots: `results/plots/`

## 5. Result Analysis

### Key Findings
1. **Low SAE overlap**: Top-k overlap between compound and constituent activations is small (Jaccard 0.11–0.14). Bootstrap means are similarly low (0.09–0.11), suggesting no dominant shared feature set.
2. **Strong compositionality**: Compound embeddings are predicted very accurately from constituents (mean cosine 0.996), well beyond the w2-only baseline (MSE 12.49 vs 5.19).
3. **Weak causal patching effect**: Patching compound residuals into &#34;washing process&#34; did not reliably increase the &#34;machine&#34; logit (mean Δlogit -0.019, p=0.75).

### Hypothesis Testing Results
- **Supports**: The compositionality probe strongly supports the hypothesis that compound meaning can be reconstructed from constituents.
- **Does not support**: Causal patching does not indicate a single strong compound-specific direction at layer 6.
- **Ambiguity**: Low SAE overlap could indicate compound-specific features, but the small washing-only latent sample count (n=10) limits confidence.

### Error Analysis
- Washing-only contexts are rare in WikiText-2, leading to only 10 usable latent samples.
- Causal patching used only 5 template pairs; effect estimates are noisy.

### Limitations
- Single model (GPT-2 small) and single SAE layer tested.
- SAE features may not be canonical or monosemantic.
- Corpus coverage for the exact compound is limited.

## 6. Conclusions

**Summary**: In GPT-2 small, &#34;washing machine&#34; does not appear to correspond to a single strong, localized feature at the tested layer. Instead, representation geometry is strongly compositional, as evidenced by the probe results.

**Implications**: Mechanistic interpretability and concept editing should treat compound concepts as distributed compositions rather than single features, at least in small GPT-style models.

**Confidence**: Moderate. The compositionality evidence is strong, but SAE overlap and causal patching are limited by sample size and single-layer scope.

## 7. Next Steps
1. Expand corpora (WikiText-103 or The Pile) to increase washing-only and compound samples.
2. Test multiple layers and SAE locations (resid_mid, resid_post_attn, mlp_post_act).
3. Repeat with a larger model and modern SAEs to test scale effects.

## References
- Elhage et al. (2022) Toy Models of Superposition
- Cunningham et al. (2023) Sparse Autoencoders Find Highly Interpretable Features in Language Models
- Wang et al. (2022) Interpretability in the Wild (IOI)
- Meng et al. (2022) Locating and Editing Factual Associations (ROME)
- Dai et al. (2021) Knowledge Neurons


════════════════════════════════════════════════════════════════════════════════
                            RESEARCH PLAN
════════════════════════════════════════════════════════════════════════════════

# Planning

## Motivation &amp; Novelty Assessment

### Why This Research Matters
Understanding how compound concepts (e.g., &#34;washing machine&#34;) are represented in LLMs is central to mechanistic interpretability and reliable concept-level interventions. If compound nouns are not stored as distinct directions, then steering, editing, and safety interventions that assume atomic directions may be miscalibrated. This work also informs compositional generalization and how meaning is built across layers.

### Gap in Existing Work
Prior work studies linear directions, superposition, and phrase compositionality, but direct tests on compound-noun concepts in residual streams are rare. SAE papers show features are not canonical and can be polysemantic, while compositionality probes focus on constituent predictability rather than feature localization for concrete compounds like &#34;washing machine&#34;.

### Our Novel Contribution
We directly test whether compound noun concepts correspond to distinct latent features or emerge from compositional interaction of constituent features, using SAE feature attribution and causal patching on real model activations.

### Experiment Justification
- Experiment 1: SAE feature activation analysis for &#34;washing&#34;, &#34;machine&#34;, and &#34;washing machine&#34; contexts to test whether a distinct compound feature exists.
- Experiment 2: Causal tracing / activation patching to measure whether intervening on constituent features reproduces the compound concept effect without a dedicated compound feature.
- Experiment 3: Compositionality probe predicting compound phrase embeddings from constituent embeddings to quantify compositional structure and compare with feature-localization evidence.

## Research Question
Where is the compound concept &#34;washing machine&#34; represented in LLMs: as a distinct direction/feature or as a composition of constituent features (&#34;washing&#34; + &#34;machine&#34;)?

## Background and Motivation
Mechanistic interpretability increasingly relies on concept directions and feature localization. However, superposition and polysemanticity suggest that not every concept corresponds to a unique direction. Compound nouns provide a concrete testbed for whether concept representations are atomic or compositional in residual streams.

## Hypothesis Decomposition
- H1: There is no single orthogonal residual-stream direction uniquely associated with &#34;washing machine&#34; across contexts.
- H2: Activation patterns for &#34;washing machine&#34; can be approximated by combining constituent features (&#34;washing&#34; and &#34;machine&#34;).
- H3: Interventions on constituent features reproduce behavioral effects similar to the compound without requiring a dedicated compound feature.

## Proposed Methodology

### Approach
Use mechanistic interpretability tools (TransformerLens + SAE features) to analyze activations and causal impact for &#34;washing machine&#34; contexts. Combine SAE feature inspection with causal patching and compositionality probes.

### Experimental Steps
1. Data collection: Extract contexts containing &#34;washing&#34;, &#34;machine&#34;, and &#34;washing machine&#34; from WikiText-2; create matched control contexts. Rationale: Controlled contexts reduce confounds.
2. SAE feature analysis: Use pretrained SAEs (openai/sparse_autoencoder) to obtain latent activations; identify top-activating features for each condition. Rationale: Test existence of a compound-specific latent.
3. Causal tracing: Patch activations or SAE latents from constituent contexts into compound contexts (and vice versa). Rationale: Test whether constituent features are sufficient to induce compound behavior.
4. Compositionality probe: Train linear/affine probes to predict compound phrase embedding from constituent embeddings. Rationale: Quantify compositional structure of phrase representations.

### Baselines
- Randomized model or shuffled activations baseline for SAE metrics.
- Linear probe using only constituent embedding averages.
- Single-layer SAE vs. multi-layer SAE features (if available).

### Evaluation Metrics
- SAE reconstruction loss and sparsity by condition.
- Feature overlap metrics (Jaccard of top-k features) across conditions.
- Causal effect size from patching (logit diff / probability change).
- Probe reconstruction error (cosine distance / MSE).

### Statistical Analysis Plan
- Bootstrap confidence intervals for overlap and patching effects.
- Paired t-tests or Wilcoxon signed-rank tests for condition comparisons.
- Significance level α = 0.05 with FDR correction for multiple comparisons.

## Expected Outcomes
- Support hypothesis if compound contexts do not yield unique SAE features and if constituent feature patching reproduces compound effects.
- Refute if a consistent, unique compound feature dominates and cannot be replicated by constituent features.

## Timeline and Milestones
- Resource review and setup: 0.5 hr
- Data extraction and preprocessing: 0.5 hr
- SAE analysis + patching: 1.5 hr
- Probing experiments: 1.0 hr
- Analysis + write-up: 1.0 hr

## Potential Challenges
- Pretrained SAE compatibility with target model and tokenizer.
- Limited compound examples in WikiText-2.
- Activation patching sensitivity to hyperparameters.

## Success Criteria
- Complete SAE and causal patching experiments with quantitative comparisons.
- Clear statistical conclusion on presence/absence of compound-specific feature.
- Reproducible code and documented results in REPORT.md.


════════════════════════════════════════════════════════════════════════════════
                          LITERATURE REVIEW
════════════════════════════════════════════════════════════════════════════════

# Literature Review: Where is “Washing Machine” Stored in LLMs?

## Review Scope

### Research Question
Where (and how) are composite concepts like “washing machine” represented in transformer residual streams—via sparse monosemantic features, superposed features, or compositional mixtures of more atomic concepts (e.g., “washing” + “machine”)?

### Inclusion Criteria
- Papers on feature superposition / polysemanticity or monosemantic feature discovery
- Mechanistic interpretability papers that localize knowledge or concept-like features
- Methods that probe or edit internal representations in transformer LMs

### Exclusion Criteria
- Purely behavioral probing without access to internal activations
- Non-transformer architectures unless directly comparable

### Time Frame
2019–present (emphasis on 2021–2024)

### Sources
- arXiv
- Semantic Scholar
- Transformer Circuits (Anthropic)
- ROME project site

## Search Log

| Date | Query | Source | Results | Notes |
|------|-------|--------|---------|-------|
| 2026-02-08 | &#34;sparse autoencoders interpretable features language models&#34; | arXiv / Semantic Scholar | 10+ | High relevance to feature discovery |
| 2026-02-08 | &#34;toy models of superposition&#34; | Semantic Scholar | 1 | Core theory of superposition |
| 2026-02-08 | &#34;locating and editing factual associations in GPT&#34; | arXiv / ROME site | 1 | Causal tracing + editing |
| 2026-02-08 | &#34;interpretability in the wild IOI&#34; | arXiv | 1 | Circuit-level causal analysis |
| 2026-02-08 | &#34;knowledge neurons in pretrained transformers&#34; | arXiv | 1 | Neuron-level attribution |
| 2026-02-08 | &#34;towards monosemanticity dictionary learning&#34; | Transformer Circuits | 1 | Dictionary learning for features |
| 2026-02-08 | &#34;mathematical framework for transformer circuits&#34; | Transformer Circuits | 1 | Foundational mechanistic interpretability |

## Screening Results

| Paper | Title Screen | Abstract Screen | Full-Text | Notes |
|------|-------------|----------------|-----------|-------|
| Toy Models of Superposition | Include | Include | Include | Foundational theory for superposition |
| Sparse Autoencoders Find Highly Interpretable Features | Include | Include | Include | Direct method for feature discovery |
| Interpretability in the Wild (IOI) | Include | Include | Include | Causal tracing of behavior |
| Locating and Editing Factual Associations (ROME) | Include | Include | Include | Localization of factual knowledge |
| Knowledge Neurons | Include | Include | Include | Neuron-level attribution |
| Towards Monosemanticity | Include | Include | Include | Dictionary learning for LMs |
| Mathematical Framework for Transformer Circuits | Include | Include | Include | Background for circuit analysis |

## Paper Summaries

### Toy Models of Superposition (2022)
- **Authors**: Nelson Elhage et al.
- **Source**: arXiv 2209.10652
- **Key Contribution**: Formalizes feature superposition and polysemanticity in simplified models.
- **Methodology**: Theoretical and toy-model analysis of linear representations under sparsity constraints.
- **Datasets**: Synthetic toy tasks
- **Results**: Shows why sparse features can be superposed in a single direction when representation capacity is limited.
- **Code Available**: Yes (Anthropic / Transformer Circuits)
- **Relevance**: Provides theoretical basis for expecting composite concepts to be mixtures rather than orthogonal directions.

### Sparse Autoencoders Find Highly Interpretable Features in Language Models (2023)
- **Authors**: Hoagy Cunningham et al.
- **Source**: arXiv 2309.08600
- **Key Contribution**: SAEs can recover interpretable sparse features from transformer activations.
- **Methodology**: Train sparse autoencoders on GPT-2 activations, analyze recovered features.
- **Datasets**: GPT-2 activations from text corpora
- **Results**: Many SAE features align with human-interpretable concepts; features are sparse and compositional.
- **Code Available**: Yes (openai/sparse_autoencoder)
- **Relevance**: Directly supports searching for “washing machine” as a sparse feature or combination of features.

### Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small (2022)
- **Authors**: Kevin Wang et al.
- **Source**: arXiv 2211.00593
- **Key Contribution**: Identifies a causal circuit for IOI behavior in GPT-2 small.
- **Methodology**: Activation patching / causal tracing across layers and heads.
- **Datasets**: Synthetic IOI prompts
- **Results**: A small set of heads and MLPs are causally responsible for IOI.
- **Code Available**: Yes (Transformer Circuits)
- **Relevance**: Provides methods to locate concept-related circuits in residual streams.

### Locating and Editing Factual Associations in GPT (ROME) (2022)
- **Authors**: Kevin Meng et al.
- **Source**: arXiv 2202.05262
- **Key Contribution**: Shows factual associations are localized in specific MLP layers and can be edited.
- **Methodology**: Causal tracing + rank-one model edits.
- **Datasets**: CounterFact, zsRE
- **Results**: Edits are localized and can be evaluated with minimal side effects.
- **Code Available**: Yes (kmeng01/rome)
- **Relevance**: Suggests “washing machine” knowledge might localize to mid-layer MLPs for factual associations.

### Knowledge Neurons in Pretrained Transformers (2021)
- **Authors**: Damai Dai et al.
- **Source**: arXiv 2104.08696
- **Key Contribution**: Identifies neurons strongly associated with factual knowledge.
- **Methodology**: Gradient-based attribution and neuron editing.
- **Datasets**: LAMA-style cloze facts
- **Results**: Specific neurons influence factual outputs across prompts.
- **Code Available**: Yes (Hunter-DDM/knowledge-neurons)
- **Relevance**: Provides neuron-level localization approach for concept probes.

### Towards Monosemanticity: Decomposing Language Models With Dictionary Learning (2023)
- **Authors**: Anthropic (Transformer Circuits)
- **Source**: Transformer Circuits report
- **Key Contribution**: Dictionary learning yields more monosemantic features vs. raw neurons.
- **Methodology**: Sparse dictionary learning on residual stream activations.
- **Datasets**: LM activations
- **Results**: Monosemantic features appear in dictionary basis; supports sparse, decomposed representations.
- **Code Available**: Partial (SAE / dictionary learning tooling)
- **Relevance**: Directly tests whether “washing machine” is represented as a single feature or composition.

### A Mathematical Framework for Transformer Circuits (2021–2022)
- **Authors**: Anthropic (Transformer Circuits)
- **Source**: Transformer Circuits report
- **Key Contribution**: Defines the linear algebraic framework for analyzing attention and residual streams.
- **Methodology**: Theoretical decomposition of transformer computations.
- **Datasets**: N/A (theoretical)
- **Results**: Clarifies how information is routed and represented across layers.
- **Code Available**: N/A (report + tools in TransformerLens)
- **Relevance**: Framework needed to reason about where concept features can be stored.

## Themes and Synthesis

### Theme 1: Superposition and Polysemanticity
- Toy Models of Superposition argues that capacity constraints lead to feature superposition, motivating sparse decompositions.

### Theme 2: Sparse Feature Discovery
- SAEs and dictionary learning (Sparse Autoencoders, Towards Monosemanticity) find interpretable features in residual streams.

### Theme 3: Causal Localization
- IOI and ROME show that specific heads/MLPs carry causal responsibility for behaviors and knowledge.

### Theme 4: Neuron-Level Attribution
- Knowledge Neurons shows individual neuron sets can be linked to factual knowledge.

## Research Gaps
- Limited direct evidence for compositional concepts like “washing machine” in residual streams.
- Need explicit comparison between atomic (“washing”, “machine”) vs. composite (“washing machine”) feature activations.
- Sparse feature methods are promising but need targeted compositionality probes.

## Recommendations for Our Experiment

- **Recommended datasets**: WikiText-2 (fast activation sampling), LAMBADA (long-context prompts), CounterFact (knowledge probes).
- **Recommended baselines**: Neuron attribution (Knowledge Neurons), causal tracing (ROME), IOI-style activation patching.
- **Recommended metrics**: Feature activation sparsity, causal effect size from patching, concept separation score (e.g., cosine separation of “washing” vs “washing machine”).
- **Methodological considerations**:
  - Compare concept activation under isolated vs. composite prompts.
  - Use SAE features as candidate concept directions, then test causal impact via patching/ablation.
  - Analyze across multiple layers and resid locations (pre/post MLP).


════════════════════════════════════════════════════════════════════════════════
                          PAPER REQUIREMENTS
════════════════════════════════════════════════════════════════════════════════

Generate a complete academic paper with the following structure:

1. TITLE
   - Clear, specific, informative
   - Should convey main finding or contribution

2. ABSTRACT (150-250 words)
   - Problem statement
   - Approach
   - Key results
   - Significance

3. INTRODUCTION
   - Research problem and motivation
   - Gap in existing work
   - Our contribution (be specific)
   - Paper organization

4. RELATED WORK
   - Organized by theme/approach
   - Position our work relative to prior work
   - Cite papers from literature review

5. METHODOLOGY
   - Clear description of approach
   - Experimental setup
   - Datasets used
   - Evaluation metrics
   - Baselines

6. RESULTS
   - Present results with tables and figures
   - Statistical analysis
   - Comparison to baselines
   - Ablation studies (if applicable)

7. DISCUSSION
   - Interpretation of results
   - Limitations
   - Broader implications

8. CONCLUSION
   - Summary of contributions
   - Key findings
   - Future work

9. REFERENCES
   - BibTeX format
   - All cited papers

════════════════════════════════════════════════════════════════════════════════
                          OUTPUT FORMAT
════════════════════════════════════════════════════════════════════════════════

Create a MODULAR LaTeX project with the following directory structure:

paper_draft/
├── main.tex              # Main file that imports all sections
├── references.bib        # BibTeX references
├── sections/
│   ├── abstract.tex      # Abstract content
│   ├── introduction.tex  # Introduction section
│   ├── related_work.tex  # Related work section
│   ├── methodology.tex   # Methodology section
│   ├── results.tex       # Results section
│   ├── discussion.tex    # Discussion section
│   └── conclusion.tex    # Conclusion section
├── figures/              # Directory for any generated figures
├── tables/               # Directory for complex standalone tables
└── appendix/             # Directory for appendix sections (if needed)

INSTRUCTIONS:
1. First, create the directory structure above (mkdir -p paper_draft/sections paper_draft/figures paper_draft/tables paper_draft/appendix)
2. Write main.tex using the EXACT preamble for NEURIPS:

   \documentclass{article}
   \usepackage[final]{neurips_2025}  % NEURIPS style (neurips_2025.sty is in paper_draft/)
   \usepackage[hidelinks]{hyperref}  % REQUIRED: clickable links
   \usepackage{booktabs}  % REQUIRED: professional tables
   \usepackage{graphicx}
   \usepackage{amsmath,amssymb}

   % Import command files
   \input{commands/math}
   \input{commands/general}
   \input{commands/macros}

   % Use this bibliography style:
   \bibliographystyle{plainnat}

   - Use \input{sections/...} to include each section
   - Use \bibliography{references} for references
3. Write each section file with COMPLETE content (no placeholders)
4. Each section file should include its \section{} command
5. Write references.bib with all citations in BibTeX format
6. After writing all files, compile the paper:
   cd paper_draft && pdflatex -interaction=nonstopmode main.tex && bibtex main && pdflatex -interaction=nonstopmode main.tex && pdflatex -interaction=nonstopmode main.tex

This modular structure allows humans to easily:
- Edit individual sections without navigating a large file
- Track changes per section
- Reuse sections across different paper versions

════════════════════════════════════════════════════════════════════════════════
                          QUALITY REQUIREMENTS
════════════════════════════════════════════════════════════════════════════════

- Academic tone throughout
- All claims must be supported by data from the experiment report
- Proper citations using \cite{} commands
- Clear figures and tables with proper captions
- NO placeholder text - every section must have real content
- The paper MUST compile without errors
- If compilation fails, debug and fix the LaTeX errors

════════════════════════════════════════════════════════════════════════════════
                          LAB WRITING STYLE
════════════════════════════════════════════════════════════════════════════════

Follow these lab-specific conventions to match our paper style:

1. LANGUAGE STYLE:
   - Use active voice: "We propose", "We examine", "We focus on"
   - Be direct and confident: "Our main question is...", "We hypothesize that..."
   - State things clearly and simply - prefer plain language over jargon
   - Use bold questions as paragraph organizers: {\bf what is X?}
   - Include specific quantitative claims: "8.97% over baseline"
   - Avoid fancy wording: "utilize" → "use", "facilitate" → "help"

2. INTRODUCTION STRUCTURE:
   - Engaging hook (get to the point quickly)
   - Problem importance
   - Gap identification
   - Your approach with method figure reference
   - Quantitative preview of results
   - Contribution bullets (3-4 items, action verbs)

3. CONTRIBUTION LISTS:
   \begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
       \item We propose...
       \item We conduct...
       \item We complement...
   \end{itemize}

4. MODULAR COMMANDS STRUCTURE:
   Create paper_draft/commands/ directory with:
   - math.tex: Math notation macros (copy from templates/paper_writing/commands/)
   - general.tex: Formatting macros (copy from templates/paper_writing/commands/)
   - macros.tex: Project-specific term definitions

   In main.tex, include:
   \input{commands/math}
   \input{commands/general}
   \input{commands/macros}

5. REFERENCE CONVENTIONS:
   Use reference macros from math.tex:
   - \figref{fig:name} for "figure 1" (lowercase, in-sentence)
   - \Figref{fig:name} for "Figure 1" (capitalized, start of sentence)
   - \secref{sec:name} for "section 2"

6. TEXT FORMATTING:
   - Use \para{Header text} for bold paragraph headers
   - Define method/dataset names with \textsc and \xspace:
     \newcommand{\methodname}{\textsc{MethodName}\xspace}

7. TABLE FORMATTING:
   - Use booktabs package (no vertical lines)
   - Use \resizebox{\textwidth}{!}{...} for wide tables
   - Use @{} to remove padding at table edges
   - Use \cmidrule(lr){x-y} for sub-headers
   - Use \textsc{} for dataset/method names in headers
   - Bold best results with {\bf ...}

8. FIGURE FORMATTING:
   - Use 0.32\textwidth for 3-column subfigures
   - Use 0.95\linewidth for full-width figures
   - Use \input{figures/legend} for shared legends
   - Write self-contained captions explaining key observations

9. RESULTS PRESENTATION:
   - Define \increase and \decrease for colored arrows (green up, red down)
   - Bold best results in tables
   - Report confidence intervals when available

10. ALGORITHM STYLING:
    - Use algpseudocode with [noend]
    - Use \triangleright for comments

11. HYPERLINKS (REQUIRED):
    - Always use \usepackage[hidelinks]{hyperref} or with colored links
    - All citations, section refs, figure refs, table refs must be clickable
    - This is essential for reader navigation

════════════════════════════════════════════════════════════════════════════════
                          WORKFLOW: REVIEW AND REFLECT
════════════════════════════════════════════════════════════════════════════════

Before calling finish, you MUST complete these review steps:

1. REVIEW RESOURCES (at the start):
   - Read templates/skills/paper-writer/SKILL.md for detailed guidance
   - Study templates/paper_writing/lab_style_guide.md for style conventions
   - Browse paper_examples/ for formatting and language patterns

2. SELF-REFLECTION (before finishing):
   After writing all sections, review your work against these criteria:

   LANGUAGE CHECK:
   - [ ] Is the writing clear and jargon-free?
   - [ ] Are claims specific with quantitative support?
   - [ ] Is active voice used throughout?

   FORMATTING CHECK:
   - [ ] Does main.tex include \input{commands/math}, \input{commands/general}, \input{commands/macros}?
   - [ ] Is hyperref package included for clickable references?
   - [ ] Do tables use booktabs (no vertical lines)?
   - [ ] Are best results bolded in tables?
   - [ ] Are figures/tables properly captioned?

   STRUCTURE CHECK:
   - [ ] Does introduction follow: hook → importance → gap → approach → preview → contributions?
   - [ ] Are contribution bullets specific with action verbs?
   - [ ] Does the paper compile without errors?

3. FIX ISSUES:
   - Address any issues found in the self-reflection
   - Re-compile and verify the PDF looks correct

Only after completing this review should you consider the paper finished.